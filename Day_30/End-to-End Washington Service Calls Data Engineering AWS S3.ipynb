{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Aj4HnekwG8sJ",
    "outputId": "57a9f5ae-d548-4b2e-e39c-6ab1337bb02e"
   },
   "outputs": [],
   "source": [
    "!pip install boto3\n",
    "# !pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mp5wMf5-LShv",
    "outputId": "fd2ca484-e7de-4386-d453-142c7eb84bfe"
   },
   "outputs": [],
   "source": [
    "!pip install opendatasets --upgrade\n",
    "!pip install mysql-connector-python==8.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmkLecvn-Avw"
   },
   "source": [
    "- [Hands-on Cloud S3 - Tutorial](https://hands-on.cloud/boto3-s3-tutorial/)\n",
    "- [Analyticsvidhya S3 Tutorial](https://www.analyticsvidhya.com/blog/2022/12/using-aws-s3-with-python-boto3/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sX9_jQEPG8sM"
   },
   "outputs": [],
   "source": [
    "# interact with aws\n",
    "import boto3\n",
    "\n",
    "# system manipulation\n",
    "import os\n",
    "import io\n",
    "import pathlib\n",
    "from glob import glob\n",
    "import uuid\n",
    "from getpass import getpass\n",
    "\n",
    "# to handle  data retrieval\n",
    "import urllib3\n",
    "from urllib3 import request\n",
    "\n",
    "# to handle certificate verification\n",
    "import certifi\n",
    "\n",
    "# to manage json data\n",
    "import json\n",
    "# import geopandas as gpd\n",
    "\n",
    "# for pandas dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# datetime, argparse\n",
    "import time\n",
    "import datetime\n",
    "from math import ceil\n",
    "import argparse\n",
    "\n",
    "# a simple logging message\n",
    "import logging\n",
    "\n",
    "# using request library\n",
    "import requests\n",
    "\n",
    "import opendatasets as od"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_1ARSI1wlrF"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xlDX9H4ntFyi"
   },
   "outputs": [],
   "source": [
    "# config our logging to write to an output file\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    # filename='log.log',\n",
    "                    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "                    # filemode='w',\n",
    "                    handlers=[\n",
    "                                logging.FileHandler(\"debug.log\"),\n",
    "                                logging.StreamHandler()\n",
    "                            ],\n",
    "                    force=True # logging.basicConfig can be run just once, we use \"force=True\" to reset any previous configuration\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6UYG51dRXGpY"
   },
   "outputs": [],
   "source": [
    "os.environ[\"AWS_DEFAULT_REGION\"] = 'us-east-2' # change to your own region\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = getpass('Enter AWS Access Key ID: ') #'*********AHZ4IVO******'\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = getpass('Enter AWS Secret Access Key: ') #'****4W4*******QW1W*****************'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C4U6fUeSfz16"
   },
   "outputs": [],
   "source": [
    "def create_bucket_name(bucket_prefix):\n",
    "    return ''.join([bucket_prefix, str(uuid.uuid4())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pi9cFEM4g0Zm"
   },
   "outputs": [],
   "source": [
    "def create_bucket(S3_BUCKET_PREFIX, s3):\n",
    "    session = boto3.session.Session()\n",
    "    AWS_REGION = session.region_name\n",
    "    \n",
    "    S3_BUCKET_NAME = create_bucket_name(S3_BUCKET_PREFIX)\n",
    "    if AWS_REGION == 'us-east-1':\n",
    "        response = s3.create_bucket(Bucket=S3_BUCKET_NAME)\n",
    "    else:\n",
    "        location = {'LocationConstraint': AWS_REGION}\n",
    "        response = s3.create_bucket(Bucket=S3_BUCKET_NAME,\n",
    "                                    CreateBucketConfiguration=location)\n",
    "    print(f\"Amazon S3 {S3_BUCKET_NAME} bucket has been created in {AWS_REGION}\")\n",
    "    return S3_BUCKET_NAME, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-UKdNtaO7pt4"
   },
   "outputs": [],
   "source": [
    "AWS_REGION = boto3.session.Session().region_name\n",
    "\n",
    "S3_BUCKET_PREFIX = \"service-call-dc-\"\n",
    "\n",
    "BASE_DIR = os.getcwd() #pathlib.Path(__file__).parent.resolve()\n",
    "\n",
    "s3_client = boto3.client(\"s3\", region_name=AWS_REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C8fQuLgdSVV8",
    "outputId": "84416098-056f-4990-99db-a1c944d9dcf3"
   },
   "outputs": [],
   "source": [
    "S3_BUCKET_NAME, response = create_bucket(S3_BUCKET_PREFIX, s3_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vi3w58S8G8sO"
   },
   "outputs": [],
   "source": [
    "s3_resource = boto3.resource(\n",
    "    service_name='s3',\n",
    "    region_name=AWS_REGION,\n",
    "    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    ")\n",
    "# S3_BUCKET_NAME, response = create_bucket(S3_BUCKET_PREFIX, s3_resource)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1z019u3EkqGB"
   },
   "source": [
    "## Listing Existing Buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-iew2OrfkkY9"
   },
   "source": [
    "### Listing S3 Buckets using Boto3 client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GJ-KLo3OkTv0",
    "outputId": "d4a9e835-90cc-4c85-f812-dec33daf14fc"
   },
   "outputs": [],
   "source": [
    "response = s3_client.list_buckets()\n",
    "print(\"Listing Amazon S3 Buckets:\")\n",
    "for bucket in response['Buckets']:\n",
    "    print(f\"-- {bucket['Name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSsPdPqpkeY7"
   },
   "source": [
    "### Listing S3 Buckets using Boto3 resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FuQORO21kHmx",
    "outputId": "2bf56b6c-dade-4cf9-b8fd-fb0555e69493"
   },
   "outputs": [],
   "source": [
    "# Print out bucket names\n",
    "for bucket in s3_resource.buckets.all():\n",
    "    print(bucket.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GlwEFegDkHeP"
   },
   "outputs": [],
   "source": [
    "def bucket_exists_cli(bucketName):\n",
    "    response = s3_client.list_buckets()\n",
    "    for bucket in response['Buckets']:\n",
    "        if bucketName == bucket['Name']:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def bucket_exists_res(bucket):\n",
    "    return s3_resource.Bucket(bucket) in s3_resource.buckets.all()\n",
    "\n",
    "\n",
    "def upload_path(local_directory, bucket, destination, certain_upload=False):\n",
    "\n",
    "  # enumerate local files recursively\n",
    "    for root, dirs, files in os.walk(local_directory):\n",
    "\n",
    "        for filename in files:\n",
    "\n",
    "            # construct the full local path\n",
    "            local_path = os.path.join(root, filename)\n",
    "\n",
    "            # construct the full Dropbox path\n",
    "            relative_path = os.path.relpath(local_path, local_directory)\n",
    "            s3_path = os.path.join(destination, relative_path)\n",
    "\n",
    "            if certain_upload:\n",
    "                s3_client.upload_file(local_path, bucket, s3_path)\n",
    "                return\n",
    "\n",
    "            print('Searching \"%s\" in \"%s\"' % (s3_path, bucket))\n",
    "            try:\n",
    "                s3_client.head_object(Bucket=bucket, Key=s3_path)\n",
    "                # print(\"Path found on S3! Skipping %s...\" % s3_path)\n",
    "            except:\n",
    "                print(\"Uploading %s...\" % s3_path)\n",
    "                s3_client.upload_file(local_path, bucket, s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "li2fHdhVE96x"
   },
   "source": [
    "### How to enable S3 Bucket versioning using Boto3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MdWMlz3uFEFT",
    "outputId": "1fdbd374-e8d1-4a02-ed3b-8fbaf7a7e7ea"
   },
   "outputs": [],
   "source": [
    "# s3_resource = boto3.resource(\"s3\", region_name=AWS_REGION)\n",
    "\n",
    "def enable_version(bucket_name):\n",
    "    versioning = s3_resource.BucketVersioning(bucket_name)\n",
    "    versioning.enable()\n",
    "    print(f'S3 Bucket versioning: {versioning.status}')\n",
    "\n",
    "buckets = s3_resource.buckets.all()\n",
    "\n",
    "for mybucket in buckets:\n",
    "    enable_version(mybucket.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ML5gM8gd6Wel"
   },
   "source": [
    "## Retriving From API And Normlizing Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJ1jA81w60Rd"
   },
   "outputs": [],
   "source": [
    "def flat_items(d, key_separator='.'):\n",
    "    \"\"\"\n",
    "    Flattens the dictionary containing other dictionaries like here: https://stackoverflow.com/questions/6027558/flatten-nested-python-dictionaries-compressing-keys\n",
    "\n",
    "    >>> example = {'a': 1, 'c': {'a': 2, 'b': {'x': 5, 'y' : 10}}, 'd': [1, 2, 3]}\n",
    "    >>> flat = dict(flat_items(example, key_separator='_'))\n",
    "    >>> assert flat['c_b_y'] == 10\n",
    "    \"\"\"\n",
    "    for k, v in d.items():\n",
    "        if type(v) is dict:\n",
    "            for k1, v1 in flat_items(v, key_separator=key_separator):\n",
    "                yield key_separator.join((k, k1)), v1\n",
    "        else:\n",
    "            yield k, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u76lmSjAPNV8",
    "outputId": "affbb399-9d0e-48d8-d1b8-d0947b34366e"
   },
   "outputs": [],
   "source": [
    "dataset_url = 'https://opendata.arcgis.com/api/v3/datasets/14faf3d4bfbe4ca4a713bf203a985151_0/downloads/data?format=geojson&spatialRefId=4326&where=1%3D1'\n",
    "url_name = dataset_url.split('/')[-1]\n",
    "\n",
    "od.download(dataset_url, 'data')\n",
    "\n",
    "# Source\n",
    "src = f'data/{url_name}'\n",
    "\n",
    "# Destination\n",
    "dest = 'data/All_311_City_Service_Requests_-_Last_30_Days.geojson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r6jG5fEYta4a"
   },
   "outputs": [],
   "source": [
    "data = pd.read_json('./data/All_311_City_Service_Requests_-_Last_30_Days.geojson', orient='index').T.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "FDCce6I860LF",
    "outputId": "f621a2a1-a6ce-431d-bb1e-fe3eccec91e2"
   },
   "outputs": [],
   "source": [
    "service_req_df_combine = []\n",
    "serviec_req_cols = []\n",
    "strt = time.time()\n",
    "\n",
    "for item in data['features'][0]:\n",
    "    if len(serviec_req_cols) < 1:\n",
    "        serviec_req_cols = [k for k,_ in list(flat_items(item, key_separator='.'))]\n",
    "    service_req_df_combine.append([v for _,v in list(flat_items(item, key_separator='.'))])\n",
    "\n",
    "service_req_df_json = pd.DataFrame(service_req_df_combine, columns=serviec_req_cols)\n",
    "endt = time.time()\n",
    "print('Time Taken: ', endt - strt)\n",
    "service_req_df_json.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qtkXRQBALg5"
   },
   "source": [
    "## How to upload file to S3 Bucket using Boto3?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZR31PLDSJzuz"
   },
   "source": [
    "#### Read the AWS S3 file to Pandas DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9UZaU0tnDqV_",
    "outputId": "7ad275de-8655-47fa-abb0-bb221ddacee5"
   },
   "outputs": [],
   "source": [
    "working_buckets = list(s3_resource.buckets.all())\n",
    "working_buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZ87e4U7K_ql"
   },
   "outputs": [],
   "source": [
    "S3_BUCKET_NAME = working_buckets[0].name\n",
    "data_to_upload = {'All_311_City_Service_Requests_-_Last_30_Days.json' : service_req_df_json} #, 'Roadway_Block.geojson':road_block_df_json}\n",
    "\n",
    "def json_stream(value, S3_BUCKET_NAME, FileName):\n",
    "    json_buffer = io.StringIO()\n",
    "\n",
    "    # Create dataframe and convert to pandas\n",
    "    value.to_json(json_buffer, orient='records')\n",
    "\n",
    "    response = s3_client.put_object(Body=json_buffer.getvalue(),\n",
    "                                    Bucket=S3_BUCKET_NAME,\n",
    "                                    Key=FileName)\n",
    "\n",
    "def csv_stream(value, S3_BUCKET_NAME, FileName):\n",
    "    csv_buffer = io.StringIO()\n",
    "\n",
    "    # Create dataframe and convert to pandas\n",
    "    value.to_csv(csv_buffer, index=False)\n",
    "\n",
    "    response=s3_client.put_object(Body=csv_buffer.getvalue(),\n",
    "                                Bucket=S3_BUCKET_NAME,\n",
    "                                Key=FileName)\n",
    "\n",
    "\n",
    "def upload_to_s3(S3_BUCKET_NAME, data_to_upload, folder_prefix, format='csv'):\n",
    "    for key, value in data_to_upload.items():\n",
    "        FileName = f'{folder_prefix}_{datetime.datetime.now().date()}/{key}'\n",
    "        if format=='csv':\n",
    "            csv_stream(value, S3_BUCKET_NAME, FileName)\n",
    "        else:\n",
    "            json_stream(value, S3_BUCKET_NAME, FileName)\n",
    "\n",
    "    \n",
    "def download_from_s3(S3_BUCKET_NAME, folder_prefix, format='csv'):\n",
    "    my_dict = {}\n",
    "    for key in s3_client.list_objects(Bucket=S3_BUCKET_NAME, Prefix=folder_prefix)['Contents']:\n",
    "        print(key['Key'])\n",
    "        df_name = key['Key'].split('/')[-1].split('.')[0].split('-')[0]\n",
    "\n",
    "        obj = s3_client.get_object(Bucket= S3_BUCKET_NAME ,\n",
    "                                   Key = key['Key'])\n",
    "\n",
    "        if format == 'csv':\n",
    "            my_dict[df_name]  = pd.read_csv(io.BytesIO(obj['Body'].read()), parse_dates=True, infer_datetime_format=True, encoding='utf8')\n",
    "        else:\n",
    "            my_dict[df_name]  = pd.read_json(io.BytesIO(obj['Body'].read()), orient='records', encoding='utf8')\n",
    "        print(f\"{df_name} downloaded successfully\")\n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MHZzIxF3bR1k",
    "outputId": "9c6607f5-0eeb-48d4-e1ab-8fdb89689b30"
   },
   "outputs": [],
   "source": [
    "folder_prefix='01_data_collection/01_raw_data_'\n",
    "upload_to_s3(S3_BUCKET_NAME, data_to_upload, folder_prefix=folder_prefix, format='json')\n",
    "my_dict = download_from_s3(S3_BUCKET_NAME, folder_prefix, format='json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ta10qaE1laSh"
   },
   "outputs": [],
   "source": [
    "service_df = my_dict['All_311_City_Service_Requests_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 678
    },
    "id": "pHmdftG9sohQ",
    "outputId": "582c597d-9a51-4149-bbe2-b92ec24f72c4"
   },
   "outputs": [],
   "source": [
    "service_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJpiP38JzCM5"
   },
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "id": "KMJ9WguP0xeY",
    "outputId": "e2fcc1f5-884f-4f7a-ca18-54c360fe44c9"
   },
   "outputs": [],
   "source": [
    "service_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3YkpojJ9KZww"
   },
   "outputs": [],
   "source": [
    "# Column Cleaning\n",
    "service_df.columns = [col.split('.')[-1].lower() if 'geometry' not in col else col.replace('.', '_') for col in service_df.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2aGIXjnzHY7"
   },
   "source": [
    "#### Data format Revision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uGDM40ujzdka",
    "outputId": "1fedeff1-db0b-4333-d8c7-48bbbeff5792"
   },
   "outputs": [],
   "source": [
    "service_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Sa33qDo4K7j"
   },
   "outputs": [],
   "source": [
    "col_types = {'adddate': 'datetime64[ns]', 'resolutiondate':'datetime64[ns]', 'serviceduedate':'datetime64[ns]',\n",
    "             'serviceorderdate':'datetime64[ns]', 'inspectionflag':'str', 'inspectiondate':'datetime64[ns]',\n",
    "             'inspectorname':'str', 'status_code':'str', 'zipcode':'str', 'ward':'str', 'creator':'str',\n",
    "             'created':'datetime64[ns]', 'editor':'str', 'edited':'datetime64[ns]'}\n",
    "\n",
    "service_df = service_df.astype(col_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DAlALlaSYXjK",
    "outputId": "4e5136ae-0498-456d-cfdb-e4a1e838cc3a"
   },
   "outputs": [],
   "source": [
    "service_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ztMhaXOI8ch9"
   },
   "outputs": [],
   "source": [
    "dtype_dict = service_df.dtypes.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JaMU1HAiZa6d",
    "outputId": "fa585958-ee16-4068-cb38-e0425da00de1"
   },
   "outputs": [],
   "source": [
    "dtype_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUlDpeaXzM3C"
   },
   "source": [
    "#### Adddress Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "FLkZ631Uzec5",
    "outputId": "929ad84b-a6f4-4b13-9477-0f275e3a1799"
   },
   "outputs": [],
   "source": [
    "# No need for address parsin, it has been taken care of already\n",
    "service_df.loc[:,['streetaddress', 'xcoord', 'ycoord', 'latitude', 'longitude',\n",
    "                  'city', 'state', 'zipcode', 'maraddressrepositoryid', 'ward']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FAUr96KEA9hQ",
    "outputId": "427153bb-c497-41bb-85cb-aae7b2f12391"
   },
   "outputs": [],
   "source": [
    "S3_BUCKET_NAME = working_buckets[0].name\n",
    "data_to_upload = {'Staging_All_311_City_Service_Requests_-_Last_30_Days.csv': service_df}\n",
    "\n",
    "folder_prefix='first_staging/02_staging_phase_'\n",
    "upload_to_s3(S3_BUCKET_NAME, data_to_upload, folder_prefix=folder_prefix, format='csv')\n",
    "my_dict = download_from_s3(S3_BUCKET_NAME, folder_prefix, format='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "id": "sFcok2LAC44z",
    "outputId": "42f24878-0d14-49fc-fce8-63f888a32ee8"
   },
   "outputs": [],
   "source": [
    "staged_service_df = my_dict['Staging_All_311_City_Service_Requests_']\n",
    "staged_service_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7JA7xEPLZ2hF"
   },
   "outputs": [],
   "source": [
    "staged_service_df = staged_service_df.astype(dtype_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OS0BeztYTB6w",
    "outputId": "80b216af-7d15-4329-c513-cbc21f190046"
   },
   "outputs": [],
   "source": [
    "staged_service_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9fMElkYzQ9U"
   },
   "source": [
    "#### Data Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ir9iIQ_zzLzo"
   },
   "source": [
    "These are removed since they contain no value\n",
    "- ['status_code', 'inspectionflag', 'inspectiondate', 'inspectorname', 'details', 'gis_id']\n",
    "\n",
    "No records of them in the MetaData\n",
    "- globalid, creator, created, editor, edited,\n",
    "\n",
    "Singular value\n",
    "- ['type', 'city', 'state', 'geometry_type]\n",
    "\n",
    "Important Notice:\n",
    "1. We would not remove status_code, reason is because \"This field was replaced by SERVICEORDERSTATUS for requests resolved as of 6/15/19. Prior to this date, one or both fields may be used\".\n",
    "2. details column contains Information about the action expected to fulfill the request or otherwise address the information reported. This column might be important for text processing in future so won't be removed.\n",
    "3. city, state though contains single value, but might be important when working with service requestd of all cty and state in the US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 721
    },
    "id": "8_Ji8DANFOTs",
    "outputId": "ca6103e2-43dc-4c52-e937-a5f0eb440926"
   },
   "outputs": [],
   "source": [
    "drop_cols_service_df = staged_service_df.drop(['type', 'inspectionflag', 'inspectiondate', 'inspectorname', 'gis_id', 'globalid', 'creator', 'created', 'editor', 'edited', 'geometry_type'], axis=1)\n",
    "drop_cols_service_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAKidx8BzHQ8"
   },
   "source": [
    "#### Data De_duplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1GhCRg0F-Ur"
   },
   "source": [
    "Duplicte columns with Lat and Long\n",
    "- coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "3nQLqZZtIh09",
    "outputId": "fd413250-3ce2-48c3-ab94-ff2e65c9d665"
   },
   "outputs": [],
   "source": [
    "de_duplicate_service_df = drop_cols_service_df.drop(['geometry_coordinates'], axis=1)\n",
    "de_duplicate_service_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLzxMaoKaNwg"
   },
   "outputs": [],
   "source": [
    "dtype_dict = de_duplicate_service_df.dtypes.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fm5mUZWtJCo_",
    "outputId": "b181734f-5ed0-4054-83e3-286fa6509caa"
   },
   "outputs": [],
   "source": [
    "S3_BUCKET_NAME = working_buckets[0].name\n",
    "data_to_upload = {'Final_Staging_All_311_City_Service_Requests_-_Last_30_Days.csv': de_duplicate_service_df}\n",
    "\n",
    "folder_prefix='final_staging/03_final_staging_phase_'\n",
    "upload_to_s3(S3_BUCKET_NAME, data_to_upload, folder_prefix=folder_prefix, format='csv')\n",
    "my_dict = download_from_s3(S3_BUCKET_NAME, folder_prefix, format='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmlcaUHYI1mB"
   },
   "source": [
    "Creating Facts and Dimension Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWoA_4ncJDRt"
   },
   "outputs": [],
   "source": [
    "final_staging_service_df = my_dict['Final_Staging_All_311_City_Service_Requests_']\n",
    "final_staging_service_df = final_staging_service_df.astype(dtype_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pS-kyQMGl7UI",
    "outputId": "981cdd21-d964-40c5-ee5b-2b3f921f9d2a"
   },
   "outputs": [],
   "source": [
    "final_staging_service_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "9rSJcOZel7O-",
    "outputId": "37665b1c-7d12-4af3-a013-2d1e5579e549"
   },
   "outputs": [],
   "source": [
    "final_staging_service_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "PIx94_MKl7Ga",
    "outputId": "f6db4544-7d1e-49d6-fcb1-a74aabe46782"
   },
   "outputs": [],
   "source": [
    "service_info = final_staging_service_df.loc[:,['servicecode', 'servicecodedescription', 'servicetypecodedescription', 'organizationacronym']]\n",
    "service_info.drop_duplicates(subset=\"servicecode\", inplace=True)\n",
    "service_info = service_info.reset_index(drop=True)\n",
    "service_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RFNc7EX9anB2",
    "outputId": "ac7ebdb9-d26a-440f-c8b1-d6eb150a62a9"
   },
   "outputs": [],
   "source": [
    "service_info.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k2lR4hGkUcAv"
   },
   "outputs": [],
   "source": [
    "directory = 'output'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "service_info.to_csv('output/dimServiceType.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "5Lij_t0ktCiD",
    "outputId": "574d2e22-6bfe-46cd-ac1e-c80605ccaeaa"
   },
   "outputs": [],
   "source": [
    "def get_time(x):\n",
    "    if x >= 5 and x < 12:\n",
    "        return 'morning'\n",
    "    elif x >= 12 and x < 17:\n",
    "        return 'afternoon'\n",
    "    elif x >= 17 and x < 21:\n",
    "        return 'evening'\n",
    "    else:\n",
    "        return 'night'\n",
    "\n",
    "get_time(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "fSPisG_xx6h-",
    "outputId": "f4dd5f18-ab21-4cb9-e852-4a913b930c33"
   },
   "outputs": [],
   "source": [
    "start_date = final_staging_service_df['adddate'].min() - pd.DateOffset(years= 1)\n",
    "end_date = final_staging_service_df['serviceduedate'].max() + pd.DateOffset(years= 1)\n",
    "\n",
    "request_date = pd.DataFrame(pd.date_range(start=start_date,\n",
    "                           end=end_date, freq='1H', normalize=True), columns=['Date']) #final_staging_service_df['serviceduedate'].max()\n",
    "\n",
    "request_date['year'] = request_date['Date'].dt.year\n",
    "request_date['months'] = request_date['Date'].dt.month\n",
    "request_date['day'] = request_date['Date'].dt.day\n",
    "request_date['week'] = request_date['Date'].dt.isocalendar().week\n",
    "request_date['dayofweek'] = request_date['Date'].dt.dayofweek\n",
    "request_date['quarter'] = request_date['Date'].dt.quarter\n",
    "request_date['is_weekend'] = request_date['dayofweek'].apply(lambda x: True if x in [5,6] else False)\n",
    "request_date['hour'] = request_date['Date'].dt.hour\n",
    "request_date['time_of_day'] = request_date['hour'].apply(lambda x: get_time(x))\n",
    "\n",
    "request_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8SviA-udawlM",
    "outputId": "8ef9c14d-3f84-448b-b758-b80fea7c60b8"
   },
   "outputs": [],
   "source": [
    "request_date.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XAEk-IjvU9Y5"
   },
   "outputs": [],
   "source": [
    "request_date.to_csv('output/dimDate.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "id": "-u8P1mbfvmsS",
    "outputId": "202744e4-eb38-4bcf-ffeb-d1280e208014"
   },
   "outputs": [],
   "source": [
    "status_col = ['priority', 'serviceorderstatus', 'status_code',]\n",
    "\n",
    "service_status = final_staging_service_df.loc[:,status_col]\n",
    "service_status.drop_duplicates(subset=[\"priority\", \"serviceorderstatus\"], inplace=True)\n",
    "service_status.dropna(subset=['priority'], inplace=True)\n",
    "service_status = service_status.reset_index(drop=True).reset_index()\n",
    "service_status['index'] = service_status['index'].apply(lambda x : x+1)\n",
    "service_status.rename(columns={'index':'status_id'}, inplace=True)\n",
    "service_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GBzrFYfybEgO",
    "outputId": "d8c51bdd-ddeb-4e83-b462-838bcc2c5180"
   },
   "outputs": [],
   "source": [
    "service_status.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eS1obTbRVEpX"
   },
   "outputs": [],
   "source": [
    "service_status.to_csv('output/dimServiceStatus.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "uGU1x-TPzdFb",
    "outputId": "aeee8f91-6f8b-48a6-b0dd-e6f923edf9e8"
   },
   "outputs": [],
   "source": [
    "address_col = ['streetaddress', 'xcoord', 'ycoord', 'latitude', 'longitude', 'city', 'state', 'zipcode', 'maraddressrepositoryid', 'ward']\n",
    "\n",
    "address_details = final_staging_service_df.loc[:, address_col]\n",
    "address_details.drop_duplicates(subset=[\"streetaddress\"], inplace=True)\n",
    "address_details = address_details.reset_index(drop=True).reset_index()\n",
    "address_details['index'] = address_details['index'].apply(lambda x : x+1)\n",
    "address_details.rename(columns={'index':'address_id'}, inplace=True)\n",
    "address_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xktBeizJbNrV",
    "outputId": "13409ae5-2a7a-4a36-f235-3871d9db4535"
   },
   "outputs": [],
   "source": [
    "address_details.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yWPH-1i9Vady"
   },
   "outputs": [],
   "source": [
    "address_details.to_csv('output/dimLocation.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yB9AjgNvGyJJ",
    "outputId": "2d4c1e3e-d09a-4597-b603-af85ebaebded"
   },
   "outputs": [],
   "source": [
    "final_staging_service_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "id": "dnPRs_Dxu-YZ",
    "outputId": "160da56f-a402-4156-f54e-f40b7c530389"
   },
   "outputs": [],
   "source": [
    "serviceFacts = final_staging_service_df.drop(['servicecodedescription', 'servicetypecodedescription', 'organizationacronym'], axis=1)\n",
    "serviceFacts = serviceFacts.merge(service_status, left_on=['serviceorderstatus', 'priority'], right_on=['serviceorderstatus','priority'])\n",
    "serviceFacts.drop(['serviceorderstatus', 'priority','status_code_y','status_code_x'], axis=1, inplace=True)\n",
    "address_col.remove('streetaddress') # removes streetaddress from the list, because it would be used for merging the two dataframe\n",
    "serviceFacts.drop(address_col, axis=1, inplace=True)\n",
    "serviceFacts = serviceFacts.merge(address_details, left_on=['streetaddress'], right_on=['streetaddress'])\n",
    "address_col.insert(0,'streetaddress') # insertng the streetaddress so the column could be droppped\n",
    "serviceFacts.drop(address_col, axis=1, inplace=True)\n",
    "\n",
    "# below code changes the time to nearest hour\n",
    "serviceFacts.loc[:,['adddate', 'resolutiondate', 'serviceduedate', 'serviceorderdate']] = \\\n",
    "            serviceFacts[['adddate', 'resolutiondate', 'serviceduedate', 'serviceorderdate']].apply(lambda x: x.round('H'))\n",
    "\n",
    "# serviceFacts\n",
    "serviceFacts = serviceFacts[['servicerequestid','servicecode', 'status_id', 'address_id', 'adddate', \\\n",
    "                             'resolutiondate','serviceduedate', 'serviceorderdate', 'details', 'servicecallcount']]\n",
    "serviceFacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lZm1u8hibU-1",
    "outputId": "49f4d380-2bf9-46bb-88b7-cc9ebf266416"
   },
   "outputs": [],
   "source": [
    "serviceFacts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00HjrORHVnnv"
   },
   "outputs": [],
   "source": [
    "serviceFacts.to_csv('output/serviceFacts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVvk4N-dBav3"
   },
   "source": [
    "### Uploading multiple files to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IZd2UJQYWBfZ",
    "outputId": "81396a54-4d81-442d-d95a-3b3e5eda9187"
   },
   "outputs": [],
   "source": [
    "files = glob(f\"./output/*.csv\")\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zKLbBXlR-SAX"
   },
   "outputs": [],
   "source": [
    "# data_to_upload = {'All_311_City_Service_Requests_-_Last_30_Days.geojson' : service_req_df_json} #, 'Roadway_Block.geojson':road_block_df_json}\n",
    "\n",
    "def json_stream(value, S3_BUCKET_NAME, FileName, has_date=True):\n",
    "    json_buffer = io.StringIO()\n",
    "\n",
    "    if has_date:\n",
    "        # Create dataframe and convert to pandas\n",
    "        value.to_json(json_buffer, orient='records', date_format = 'iso', date_unit='s')\n",
    "    else:\n",
    "        # Create dataframe and convert to pandas\n",
    "        value.to_json(json_buffer, orient='index', index=True)\n",
    "\n",
    "    response = s3_client.put_object(Body=json_buffer.getvalue(),\n",
    "                                    Bucket=S3_BUCKET_NAME,\n",
    "                                    Key=FileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "8yElvHevi4F7",
    "outputId": "359d6a13-a28c-4eb4-dc3a-410e754e2566"
   },
   "outputs": [],
   "source": [
    "serviceFacts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cy1PEM2-nBX9",
    "outputId": "70af476f-1c27-40f7-a015-5ce3a61b4a78"
   },
   "outputs": [],
   "source": [
    "S3_BUCKET_NAME = working_buckets[0].name\n",
    "data_to_upload = {'dimLocation.json' : address_details,\n",
    "                  'dimDate.json' :  request_date,\n",
    "                  'serviceFacts.json' : serviceFacts,\n",
    "                  'dimServiceType.json' : service_info,\n",
    "                  'dimServiceStatus.json' : service_status}\n",
    "\n",
    "def upload_final_to_s3(S3_BUCKET_NAME, data_to_upload, folder_prefix, format='csv'):\n",
    "    for key, value in data_to_upload.items():\n",
    "        FileName = f'{folder_prefix}_{datetime.datetime.now().date()}/{key}'\n",
    "        if format=='csv':\n",
    "            csv_stream(value, S3_BUCKET_NAME, FileName)\n",
    "        elif format=='json':\n",
    "            json_stream(value, S3_BUCKET_NAME, FileName)\n",
    "        dtt = pd.DataFrame([dict(zip(value.dtypes.keys(),[str(col).replace('|','') for col in value.dtypes.values]))]).T\n",
    "        json_stream(dtt, S3_BUCKET_NAME, f\"{FileName.split('.')[0]}Datatype.json\", has_date=False)\n",
    "\n",
    "def download_final_from_s3(S3_BUCKET_NAME, folder_prefix, format='csv'):\n",
    "    my_dict = {}\n",
    "\n",
    "    for key in s3_client.list_objects(Bucket=S3_BUCKET_NAME, Prefix=folder_prefix)['Contents']:\n",
    "        print(key['Key'])\n",
    "\n",
    "        df_name = key['Key'].split('/')[-1].split('.')[0].split('-')[0]\n",
    "\n",
    "        obj = s3_client.get_object(Bucket= S3_BUCKET_NAME ,\n",
    "                                   Key = key['Key'])\n",
    "\n",
    "        if key['Key'].split('.')[-1] == 'csv':\n",
    "            my_dict[df_name]  = pd.read_csv(io.BytesIO(obj['Body'].read()), parse_dates=True, infer_datetime_format=True, encoding='utf8')\n",
    "        elif key['Key'].split('.')[-1] == 'json':\n",
    "            my_dict[df_name]  = pd.read_json(io.BytesIO(obj['Body'].read()), orient='records', encoding='utf8')\n",
    "    return my_dict\n",
    "\n",
    "folder_prefix='transformed_data/output'\n",
    "upload_final_to_s3(S3_BUCKET_NAME, data_to_upload, folder_prefix=folder_prefix, format='json')\n",
    "my_dict = download_final_from_s3(S3_BUCKET_NAME, folder_prefix, format='json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EMFn3ppWB8XS",
    "outputId": "2c06b315-e7f7-4d5e-c93d-13eed585c00b"
   },
   "outputs": [],
   "source": [
    "my_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "JbdrpvGJl8gB",
    "outputId": "bd549c81-4c40-451c-b2c0-326132bce5b9"
   },
   "outputs": [],
   "source": [
    "request_date = my_dict['dimDate']\n",
    "request_date.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V01oMu3IbsVs",
    "outputId": "470e7872-f8da-47e4-a721-e21a0fe5b5b3"
   },
   "outputs": [],
   "source": [
    "request_date.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_YQmTmAJJBE6",
    "outputId": "ee89434d-df6b-4462-af6a-b4ebdde33eee"
   },
   "outputs": [],
   "source": [
    "# ['dimDate'], ['dimDateDatatype'], ['dimLocation'], ['dimLocationDatatype'], ['dimServiceStatus'], ['dimServiceStatusDatatype'], ['dimServiceType'], ['dimServiceTypeDatatype'], ['serviceFacts'], ['serviceFactsDatatype']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EYj0PzSaRd1W",
    "outputId": "f7e1d064-60ef-483c-934d-932094285e11"
   },
   "outputs": [],
   "source": [
    "my_dict['serviceFactsDatatype'].T.to_dict()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0jBM_vmKaMz"
   },
   "outputs": [],
   "source": [
    "for key in list(my_dict.keys())[::2]:\n",
    "    my_dict[key] = my_dict[key].astype(my_dict[f'{key}Datatype'].T.to_dict()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sZeNndPKMbu6",
    "outputId": "608a7b57-5e0f-4310-d12d-a2c17752a48f"
   },
   "outputs": [],
   "source": [
    "my_dict['serviceFacts'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedf = pd.merge(my_dict['serviceFacts'], my_dict['dimServiceStatus'],on='status_id')\n",
    "mergedf = pd.merge(mergedf, my_dict['dimServiceType'], on='servicecode' )\n",
    "mergedf = pd.merge(mergedf, my_dict['dimLocation'], on='address_id' ) \n",
    "# mergedf = pd.merge(mergedf, request_date, left_on='serviceorderdate', right_on='Date' ) #, request_date\n",
    "mergedf['priority'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hsa2_oMQhcR2"
   },
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import text\n",
    "from mysql.connector import connect, Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yBoWEUvfvE6f",
    "outputId": "80c48034-9e0c-473f-a522-4fc498fd3407"
   },
   "outputs": [],
   "source": [
    "client = boto3.client(\"rds\", region_name=AWS_REGION)\n",
    "\n",
    "response = client.describe_db_instances()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23_rUQqNX8CG"
   },
   "outputs": [],
   "source": [
    "# from google.colab import userdata\n",
    "# host = userdata.get('planet_scale_host')\n",
    "# password = userdata.get('planet_scale_pwd')\n",
    "# user = userdata.get('planet_scale_username')\n",
    "# database = \"connectdatabase\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJU7o3azhSY7",
    "outputId": "a7ac151f-b05d-4702-e7c0-5b04371bc630"
   },
   "outputs": [],
   "source": [
    "# Replace these with your actual values\n",
    "dbinstance='servicedbmysql'\n",
    "endpoint = f\"{dbinstance}.***********.{AWS_REGION}.rds.amazonaws.com\" #  Create a rds mysql instance on the AWS webpage and confirm the endpoint format of your rds\n",
    "engine = \"mysql\"\n",
    "engine_version = \"8.0.28\"\n",
    "dbname = \"servicecalls\"\n",
    "username = \"root\"\n",
    "password = getpass('Enter MySQL Password: ')\n",
    "host='127.0.0.1'\n",
    "DBInstanceClass=\"db.t3.micro\"\n",
    "AllocatedStorage=20\n",
    "security_groups = getpass('Enter Security Groups separated by a comma: ').split(',') #e.g: fg-0b870chinume0by\n",
    "\n",
    "def create_rds_database():\n",
    "    client.create_db_instance(\n",
    "            DBInstanceIdentifier=dbinstance,\n",
    "            DBInstanceClass=DBInstanceClass,\n",
    "            Engine=engine,\n",
    "            EngineVersion=engine_version,\n",
    "            DBName=dbname,\n",
    "            AllocatedStorage=AllocatedStorage,\n",
    "            MasterUsername=username,\n",
    "            MasterUserPassword=password,\n",
    "            Port=3306,\n",
    "            VpcSecurityGroupIds=security_groups,\n",
    "            DeletionProtection=True\n",
    "        )\n",
    "    print(\"RDS MySQL database created!\")\n",
    "\n",
    "def check_and_create_rds_database(dbinstance='servicemysql',\n",
    "                        endpoint = f\"**.amazonaws.com\",\n",
    "                        engine = \"mysql\",\n",
    "                        engine_version = \"8.0.28\",\n",
    "                        dbname = \"databasename\",\n",
    "                        username = \"root\",\n",
    "                        password = \"admin0\",\n",
    "                        DBInstanceClass=\"db.t3.micro\",\n",
    "                        AllocatedStorage=20\n",
    "                       ):\n",
    "    client = boto3.client(\"rds\", region_name=AWS_REGION)\n",
    "\n",
    "    response = client.describe_db_instances()\n",
    "    if len(response[\"DBInstances\"])!=0:\n",
    "        for instance in response[\"DBInstances\"]:\n",
    "            if dbinstance == instance['DBInstanceIdentifier']:\n",
    "                print('DB Instance Already Exists')\n",
    "                break\n",
    "        else:\n",
    "            create_rds_database()\n",
    "    else:\n",
    "        create_rds_database()\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "def create_database(host='127.0.0.1', dbname='postdb', user='user', password='password'):\n",
    "    try:\n",
    "        # connect to default database\n",
    "        conn =  mysql.connector.connect(host=host, user=user, port='3306', password=password)\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # create servicecalls database with UTF8 encoding\n",
    "        cur.execute(f\"CREATE DATABASE IF NOT EXISTS {dbname}\")\n",
    "        \n",
    "        # close connection to default databse\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "    except Error as e:\n",
    "        print(e)\n",
    "    \n",
    "\n",
    "def create_db(localmachine=True):\n",
    "    \n",
    "    if localmachine:\n",
    "        create_database(host=host, dbname=dbname, user=username, password=password)        \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        check_and_create_rds_database(\n",
    "                            dbinstance=dbinstance,\n",
    "                            endpoint = endpoint,\n",
    "                            engine = engine,\n",
    "                            engine_version = engine_version,\n",
    "                            dbname = dbname,\n",
    "                            username = username,\n",
    "                            password = password,\n",
    "                            DBInstanceClass=DBInstanceClass,\n",
    "                            AllocatedStorage=AllocatedStorage\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_db(localmachine=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tMBaBi_vy6C1"
   },
   "outputs": [],
   "source": [
    "# Establish a connection to the database\n",
    "def create_connection(instance='dbmysql', user='admin', password=password, dbname='database', localmachine=True, endpoint=endpoint):\n",
    "    if localmachine==False:\n",
    "        connection = mysql.connector.connect(\n",
    "            host=endpoint,\n",
    "            user=user,\n",
    "            password=password,\n",
    "            database=dbname,\n",
    "            port='3306'\n",
    "        )\n",
    "    else:\n",
    "        connection =  mysql.connector.connect(host=host, database=dbname, user=user, password=password, port='3306')\n",
    "    return connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gi2wA_u1p_hc",
    "outputId": "c1679946-0608-490a-a2a5-519dbdf804df"
   },
   "outputs": [],
   "source": [
    "connection = create_connection(instance=dbinstance, user=username, dbname=dbname, localmachine=False, endpoint=endpoint)\n",
    "connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3mSnhf8n48s"
   },
   "outputs": [],
   "source": [
    "dict_df = {key: my_dict[key] for key in list(my_dict.keys())[::2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "NgU2IPPqRgaW",
    "outputId": "595087fb-0e44-4570-fcf3-d394e9df013a"
   },
   "outputs": [],
   "source": [
    "dict_df['serviceFacts'][dict_df['serviceFacts']['resolutiondate'].isna()] # View NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5wobB8Wf5Iow"
   },
   "outputs": [],
   "source": [
    "dict_df = {key: value.fillna(np.nan).replace([np.nan], [None]) for key, value in dict_df.items()} #Change NaN values to None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "m8CEqKCHSwjB",
    "outputId": "29643c50-a3fd-4579-8e3d-3f3c39097ef2"
   },
   "outputs": [],
   "source": [
    "dict_df['serviceFacts'][dict_df['serviceFacts']['resolutiondate'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2wJUjsImBYl"
   },
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "# conn = create_engine(f'mysql+mysqlconnector://{user}:{password}@{host}:3306/{database}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WspHCzyQicgR",
    "outputId": "3e44f9b7-5c92-4561-bd5b-7794a0475c13"
   },
   "outputs": [],
   "source": [
    "def get_indices(x: list, value: int) -> list:\n",
    "    \"\"\"\n",
    "    This function gets the index values of any given value from x list\n",
    "    \"\"\"\n",
    "    indices = list()\n",
    "    i = 0\n",
    "    while True:\n",
    "        try:\n",
    "            # find an occurrence of value and update i to that index\n",
    "            i = x.index(value, i)\n",
    "            # add i to the list\n",
    "            indices.append(i)\n",
    "            # advance i by 1\n",
    "            i += 1\n",
    "        except ValueError as e:\n",
    "            break\n",
    "    return indices\n",
    "\n",
    "n = [1, 2, 3, -50, -60, 0, 6, 9, -60, -60]\n",
    "print(get_indices(n, -60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r8ZPhqUGbsNS",
    "outputId": "0a6ca47d-445d-4044-b815-920cb16ed84e"
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_tables(conn):\n",
    "    str_dt = 'VARCHAR'\n",
    "    texttype = 'TEXT'\n",
    "    int_dt = 'INTEGER'\n",
    "    dec = 'DECIMAL'\n",
    "    bool_ = 'BOOL'\n",
    "    date_dt = 'TIMESTAMP'\n",
    "    \n",
    "    create_serviceFacts_table_query = f\"\"\"\n",
    "                CREATE TABLE serviceFacts(\n",
    "                    servicerequestid {str_dt}(25) NOT NULL PRIMARY KEY,\n",
    "                    servicecode {str_dt}(50),\n",
    "                    status_id {int_dt},\n",
    "                    address_id {int_dt},\n",
    "                    adddate {date_dt},\n",
    "                    resolutiondate {date_dt},\n",
    "                    serviceduedate {date_dt},\n",
    "                    serviceorderdate {date_dt},\n",
    "                    details {texttype},\n",
    "                    servicecallcount {int_dt}\n",
    "                )\n",
    "                \"\"\"\n",
    "    create_dimLocation_table_query = f\"\"\"CREATE TABLE dimLocation (\n",
    "                                        address_id {int_dt} NOT NULL PRIMARY KEY,\n",
    "                                        streetaddress {str_dt}(100),\n",
    "                                        xcoord {dec},\n",
    "                                        ycoord {dec},\n",
    "                                        latitude {dec}(12,7),\n",
    "                                        longitude {dec}(12,7),\n",
    "                                        city {str_dt}(25),\n",
    "                                        state {str_dt}(25),\n",
    "                                        zipcode {str_dt}(20),\n",
    "                                        maraddressrepositoryid {int_dt},\n",
    "                                        ward {str_dt}(10)\n",
    "                                        )\n",
    "                                        \"\"\"\n",
    "\n",
    "    create_dimDate_table_query = f\"\"\"CREATE TABLE dimDate (\n",
    "                                        Date {date_dt} NOT NULL PRIMARY KEY,\n",
    "                                        year {int_dt},\n",
    "                                        months {int_dt},\n",
    "                                        day {int_dt},\n",
    "                                        week {int_dt},\n",
    "                                        dayofweek {int_dt},\n",
    "                                        quarter {int_dt},\n",
    "                                        is_weekend {bool_},\n",
    "                                        hour {int_dt},\n",
    "                                        time_of_day {str_dt}(20)\n",
    "                                        )\n",
    "                                        \"\"\"\n",
    "\n",
    "\n",
    "    create_dimServiceType_table_query = f\"\"\"CREATE TABLE dimServiceType (\n",
    "                                        servicecode {str_dt}(20) NOT NULL PRIMARY KEY,\n",
    "                                        servicecodedescription {str_dt}(255),\n",
    "                                        servicetypecodedescription {str_dt}(255),\n",
    "                                        organizationacronym {str_dt}(50)\n",
    "                                        )\n",
    "                                        \"\"\"\n",
    "    create_dimServiceStatus_table_query = f\"\"\"CREATE TABLE dimServiceStatus (\n",
    "                                    status_id {int_dt} NOT NULL PRIMARY KEY,\n",
    "                                    priority {str_dt}(20),\n",
    "                                    serviceorderstatus {str_dt}(25),\n",
    "                                    status_code {str_dt}(10)\n",
    "                                    )\n",
    "                                    \"\"\"\n",
    "    tables = {key.split('.')[0] : [\",\".join([str(i) for i in values.columns.tolist()]), values] for key, values in dict_df.items()}\n",
    " \n",
    "    with conn.connect() as connection:\n",
    "        for key, values in dict_df.items():\n",
    "            tblname = key.split('.')[0]\n",
    "            insert_value = [\",\".join([str(i) for i in values.columns.tolist()]), values]\n",
    "            print(insert_value[1].shape)\n",
    "            connection.execute(text(f\"DROP Table IF EXISTS {tblname}\"))\n",
    "            connection.execute(text(eval(f\"create_{tblname}_table_query\")))\n",
    "            print(f'{tblname} created successfully')\n",
    "            logging.info(f\"{tblname} with a defined datatype and constraint created successfully\")\n",
    "\n",
    "            insert_into_table(table_name=tblname, df=insert_value[1], cols=insert_value[0], connection=connection)\n",
    "\n",
    "        print('All Tables Created and Data Inserted Successfully')\n",
    "\n",
    "        \n",
    "def remove_null_col_value(cols, row):\n",
    "    \"\"\"\n",
    "    This function iterates through columns and rows, then removes the index value where in both column and rows where row value is None\n",
    "    \"\"\"\n",
    "    table_col = cols.split(',')\n",
    "    idxs = get_indices(list(row), None)\n",
    "    trow = list(row)\n",
    "    for idx, j in enumerate(idxs):\n",
    "        table_col.pop(j-idx)\n",
    "        trow.pop(j-idx)\n",
    "    row = trow\n",
    "    table_col = ','.join(table_col)\n",
    "    return table_col, row\n",
    "\n",
    "\n",
    "def insert_into_table(table_name, df, cols, connection):\n",
    "    logging.info(f'Inserting Records into {table_name} table')\n",
    "   \n",
    "    # Insert DataFrame recrds one by one.\n",
    "    for i, row in df.iterrows():\n",
    "        table_col = cols\n",
    "        if None in tuple(row):\n",
    "            table_col, row = remove_null_col_value(cols, row)\n",
    "        sql = f\"INSERT INTO {table_name} ({table_col}) VALUES {tuple(row)}\"\n",
    "        connection.execute(text(sql))\n",
    "    logging.info(f'{i+1} Records successfully Inserted into {table_name} table')\n",
    "    print(f'{i+1} Records successfully Inserted')\n",
    "\n",
    "\n",
    "\n",
    "def save_to_mysql(username, password, host, database):\n",
    "    # connection = create_connection(instance=dbinstance, user=username, dbname=database)\n",
    "    # cursor = connection.cursor()\n",
    "    \n",
    "    # Connect to the database\n",
    "    conn = create_engine(f'mysql+mysqlconnector://{username}:{password}@{host}:3306/{database}')\n",
    "    logging.info('Connected to mySQL Engine')\n",
    "\n",
    "    print('Creating Tables...')\n",
    "    create_tables(conn)\n",
    "\n",
    "# save_to_mysql(username, password, hos, database)\n",
    "# save_to_mysql(username, password, endpoint, dbname)\n",
    "save_to_mysql(username, password, host, dbname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wP9BPjGqz2RK"
   },
   "outputs": [],
   "source": [
    "# Select records\n",
    "def select_records(connection, tablename):\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(f\"SELECT * FROM {tablename} LIMIT 5\")\n",
    "    rows = cursor.fetchall()\n",
    "    print(rows)\n",
    "    for row in rows:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FS1-JA5H0212",
    "outputId": "ec409947-022f-4f99-a945-f62303773aa8"
   },
   "outputs": [],
   "source": [
    "connection = create_connection(instance=dbinstance, user=username, dbname=dbname,localmachine=True)\n",
    "print(connection)\n",
    "\n",
    "\n",
    "select_records(connection, 'serviceFacts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXmWIFa9bsJw"
   },
   "outputs": [],
   "source": [
    "# if __name__==\"__main__\":\n",
    "\n",
    "#     import argparse\n",
    "#     parser = argparse.ArgumentParser()\n",
    "\n",
    "#     #database = \"connectdatabase\"\n",
    "#     #os.environ['DB_HOST'] = host # input('Enter MySQL Host Name: ')\n",
    "#     #os.environ['DB_USERNAME'] = username # input('Enter MySQL Database  Username: ')\n",
    "#     #os.environ['DB_PASSWORD'] = password #  input('Enter MySQL Database Password: ')\n",
    "\n",
    "#     # Create another group for authentication\n",
    "#     auth_group = parser.add_argument_group('Authentication', 'Login credentials')\n",
    "\n",
    "#     auth_group.add_argument(\"-u\", \"--username\", help=\"Username to connect to a database server\")\n",
    "#     auth_group.add_argument(\"-p\", \"--password\", help=\"Password to connect to a database server\")\n",
    "#     auth_group.add_argument(\"-ho\", \"--host\", help=\"Database server host\")\n",
    "#     auth_group.add_argument(\"-db\", \"--database\", help=\"Database name to be connected to\")\n",
    "#     parser.add_argument('-f', '--file', help=\"Normalize Database\", default='Signal_Blog_posts.csv')\n",
    "#     parser.add_argument('-dn', '--denormalize', type=lambda s: s.lower() in ['true', 't', 'yes', '1'], help=\"Normalize Database\", default=True)\n",
    "#     parser.add_argument('-ff', '--first_five', type=lambda s: s.lower() in ['true', 't', 'yes', '1'], help=\"Prints first five records.\", default=False)\n",
    "\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     args = vars(args)\n",
    "    # save_to_mysql(args['username'], args['password'], args['host'], args['database'], file=args['file'], denormalize=args['denormalize'], first_five=args['first_five'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQDOtmSfAJPh"
   },
   "source": [
    "## Deleting A S3 Bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIthaIpYAW1w"
   },
   "source": [
    "### Deleting RDS MySQL using Boto3 resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0hFAklPP-wJr",
    "outputId": "a8461771-6d3c-4712-9da7-d5997f143f92"
   },
   "outputs": [],
   "source": [
    "client = boto3.client('rds', AWS_REGION)\n",
    "response = client.describe_db_instances()\n",
    "\n",
    "for instance in response[\"DBInstances\"]:\n",
    "    print (\"About to delete %s\" %(instance['DBInstanceIdentifier']))\n",
    "    if input('Enter Y/N to continue: ').lower()=='y':\n",
    "        if instance['DeletionProtection']:\n",
    "            response = client.modify_db_instance(\n",
    "                                                    DBInstanceIdentifier=instance['DBInstanceIdentifier'],\n",
    "                                                    DeletionProtection=False\n",
    "                                                )\n",
    "        response = client.delete_db_instance(DBInstanceIdentifier=instance['DBInstanceIdentifier'],\n",
    "                                             SkipFinalSnapshot=True,\n",
    "                                             DeleteAutomatedBackups=True\n",
    "        )\n",
    "        print(f\"{instance['DBInstanceIdentifier']} successfully deleted!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rouAfEEKATYd"
   },
   "source": [
    "### Deleting non-empty S3 Bucket using Boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aPrGWk8r-8A5",
    "outputId": "0869082a-4373-44f3-f7d8-3fec81f20e60"
   },
   "outputs": [],
   "source": [
    "s3_resource = boto3.resource(\"s3\", region_name=AWS_REGION)\n",
    "buckets = [bucket for bucket in s3_resource.buckets.all() if S3_BUCKET_PREFIX in bucket.name]\n",
    "print(buckets)\n",
    "\n",
    "def cleanup_s3_bucket(s3_bucket):\n",
    "    # Deleting objects\n",
    "    for s3_object in s3_bucket.objects.all():\n",
    "        s3_object.delete()\n",
    "    # Deleting objects versions if S3 versioning enabled\n",
    "    for s3_object_ver in s3_bucket.object_versions.all():\n",
    "        s3_object_ver.delete()\n",
    "    print(f\"{mybucket.name} S3 Bucket cleaned up\")\n",
    "\n",
    "for mybucket in buckets:\n",
    "    s3_bucket = s3_resource.Bucket(mybucket.name)\n",
    "   \n",
    "    cleanup_s3_bucket(s3_bucket)\n",
    "    s3_bucket.delete()\n",
    "    print(f\"{mybucket.name} S3 Bucket deleted\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
