## Day 4: Data Engineering Process

The data engineering process covers a sequence of tasks that turn a large amount of raw data into a practical product meeting the needs of analysts, data scientists, machine learning engineers, and others. Today I learnt about the different stages of data engineering workflow, they include:

#### Data Collection ğŸ“š
Data can be generated by database, streaming devices, blogs or IoT deviceğŸ“². Depending on the data sources, Data engineers work with different tools to extract data from these different data sources.

#### Data Ingestion ğŸ’¨
Data ingestion moves collected/acquired data to a target system to be transformed for further analysis. Data comes in various forms and can be both structured and unstructured.

#### Data Storage ğŸ—ƒ
Data Storage is storage location of the collected data. Storage runs across the entire data engineering lifecycle, often occurring in multiple places in a data pipeline, so choosing a storage solution is key to success in the rest of the data lifecycle.

#### Data transformation â™»ï¸
Data transformation deals with changing data from its original form into a more useful form for downstream use cases. It involves removing errors and duplicates from data, normalizing it, and converting it into the needed format.

#### Data serving ğŸ“Š
Data serving delivers transformed data to end users â€” a BI platform, dashboard, or data science team.

The mechanism that automates ingestion, transformation, and serving steps of the data engineering process is known as a data pipeline ğŸ–‡.

Data flow orchestration ğŸ›  provides visibility into the data engineering process, ensuring that all tasks are successfully completed. It coordinates and continuously tracks data workflows to detect and fix data quality and performance issues.

#100DaysOfDataEngineering #DataEngineering #Data
