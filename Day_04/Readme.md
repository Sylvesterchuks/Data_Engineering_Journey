## Day 4: Data Engineering Process

The data engineering process covers a sequence of tasks that turn a large amount of raw data into a practical product meeting the needs of analysts, data scientists, machine learning engineers, and others. Today I learnt about the different stages of data engineering workflow, they include:

#### Data Collection 📚
Data can be generated by database, streaming devices, blogs or IoT device📲. Depending on the data sources, Data engineers work with different tools to extract data from these different data sources.

#### Data Ingestion 💨
Data ingestion moves collected/acquired data to a target system to be transformed for further analysis. Data comes in various forms and can be both structured and unstructured.

#### Data Storage 🗃
Data Storage is storage location of the collected data. Storage runs across the entire data engineering lifecycle, often occurring in multiple places in a data pipeline, so choosing a storage solution is key to success in the rest of the data lifecycle.

#### Data transformation ♻️
Data transformation deals with changing data from its original form into a more useful form for downstream use cases. It involves removing errors and duplicates from data, normalizing it, and converting it into the needed format.

#### Data serving 📊
Data serving delivers transformed data to end users — a BI platform, dashboard, or data science team.

The mechanism that automates ingestion, transformation, and serving steps of the data engineering process is known as a data pipeline 🖇.

Data flow orchestration 🛠 provides visibility into the data engineering process, ensuring that all tasks are successfully completed. It coordinates and continuously tracks data workflows to detect and fix data quality and performance issues.

#100DaysOfDataEngineering #DataEngineering #Data
